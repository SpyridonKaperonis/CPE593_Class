# Data Compression

Data Compression is a topic based on information theory. The number of bits required to store information involves an understanding of how to most efficiently encode information, and ultimately understanding the structure of the data.

There are a number of potential projects in this space. Any project would involve comparing the compression of all the standard techniques (Huffman, LZW, bzip2, LZMA) against whatever you come up with.

 First, you can understand and implement as much as possible of the bzip2 algorithm, which uses Burrows-Wheeler to transform the data into a form more compressible by a number of algorithms. You can then compare your implementation (if you can get it fully working) to the bzip2 utility, and compare compression of various datasets with other compressors.

Another algorithm to pick is LZMA, which is more efficient than bzip2, and sometimes compresses better, sometimes worse, but is vastly faster to decompress. LZMA is designed for optimizing when lightweight machines need to decompress the data quickly.

A third option is to demonstrate compression of a floating point data set by predicting the values, subtracting your prediction, and encoding only the deltas. The better your prediction, the more compression will be achieved. Using a predictor is an example of Kolmogorov Complexity, the idea that the ultimate compression is based on the smallest program that can recreate the data. For example, you can store the digits of pi, which appear random and could not be compressed as a result, but since there obviously is an underlying structure, you can also write a program to generate the digits of pi and use that as your "storage," effectively reducing storage of pi to the smallest program sufficient to generate it. For the floating point problem, you can get historical stock market prices of a number of securities, or weather data from many weather stations, and design an algorithm that extracts the shape of the data. This problem is related to the fitting of curves to data and computing the family that fits best. In the case of stock market or weather, there is also a strong correlation between nearby stations. When the weather gets warmer for example, two stations 100 miles apart will both rise and both fall, though there is of course difference between them. Typically every day the sun heats up the air, and every night it cools. For the stock market, big events such as stock market crashes cause many stocks to drop, and so there is a great deal of redundancy in the data set. Pick 100 or more stocks with a history of 10 years or more and attempt to compress the data. This project would attempt to extract the redundant correlation between data, and you can use a standard compressor as a final step (instead of also having to write bzip2, lzma, or one of the other compressors). Note that you don't have to predict the weather, or predict stock prices, you are taking the historical data and "predicting" based on what happened. See least squares fit project description for one way to achieve this.